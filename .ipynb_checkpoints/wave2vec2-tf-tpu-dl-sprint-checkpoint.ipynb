{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert models from pytorch to tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2jsHlsBpOY6T",
    "tags": []
   },
   "source": [
    "### **install dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5454,
     "status": "ok",
     "timestamp": 1657278018808,
     "user": {
      "displayName": "kugelblitz 1729",
      "userId": "14824418471120817982"
     },
     "user_tz": -360
    },
    "id": "2FjtlpKILn8e",
    "outputId": "eb050008-442b-4b8e-f463-7f844e6cc907"
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/mnansary/gsoc-wav2vec2.git@main\n",
    "!pip install -q git+https://github.com/vasudevgupta7/gsoc-wav2vec2@main\n",
    "!pip install -U kaggledatasets\n",
    "!pip install fsspec\n",
    "!pip install gcsfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **[there is code in this repo to convert the weights](https://github.com/vasudevgupta7/gsoc-wav2vec2/blob/main/src/convert_torch_to_tf.py)**\n",
    "\n",
    "**However this actually fail and wont serve our purpose**\n",
    "\n",
    "* the script only covers the following model conversion : \n",
    "```python\n",
    "ACCEPTABLE_HF_IDS = [\"facebook/wav2vec2-base-960h\", \"facebook/wav2vec2-base\", \"facebook/wav2vec2-large-robust\", \"facebook/wav2vec2-large-xlsr-53\"]\n",
    "```\n",
    "\n",
    "**WE CAN HOWEVER REUSE THE FUNCTIONS WITH SOME CHANGES**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection and conversion\n",
    "* The model we want to convert is **[arijitx/wav2vec2-xls-r-300m-bengali](https://huggingface.co/arijitx/wav2vec2-xls-r-300m-bengali)**\n",
    "* we inspect these two configs [tensorflow config](https://github.com/vasudevgupta7/gsoc-wav2vec2/blob/main/src/wav2vec2/config.py) and [hugging face config](https://huggingface.co/arijitx/wav2vec2-xls-r-300m-bengali/blob/main/config.json) and spot the differences\n",
    "\n",
    "| Tensorflow |Huggingface |\n",
    "|:---:|:---:|\n",
    "|num_heads: int = 12|\"num_attention_heads\": 16,|\n",
    "|num_layers: int = 12|\"num_hidden_layers\": 24,|\n",
    "|conv_bias: bool = False|\"conv_bias\": true,|\n",
    "|conv_bias: bool = False|\"conv_bias\": true,|\n",
    "|feature_extractor_norm_type: bool = \"group\"|\"feat_extract_norm\": \"layer\",|\n",
    "|hidden_size: int = 768|\"hidden_size\": 1024,|\n",
    "|intermediate_size: int = 3072|\"intermediate_size\": 4096,|\n",
    "\n",
    "**Note:we can safely ignore differences like dropout while conversion**\n",
    "\n",
    "The changes can be executed by :\n",
    "\n",
    "```python\n",
    "config = Wav2Vec2Config()\n",
    "config.num_heads=16\n",
    "config.num_layers=24\n",
    "config.conv_bias=True\n",
    "config.feature_extractor_norm_type=\"layer\"\n",
    "config.hidden_size=1024\n",
    "config.intermediate_size=4096\n",
    "```\n",
    "\n",
    "**However to avoid complexity we can use the RobustModelConfig**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorflow_version` not found.\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 2.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Union\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from typing import Union\n",
    "import tensorflow as tf\n",
    "import transformers\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from wav2vec2 import Wav2Vec2Config, RobustWav2Vec2Config, Wav2Vec2ForCTC, Wav2Vec2Model\n",
    "\n",
    "\n",
    "SUFFIX = \":0\"\n",
    "MAPPING = (\n",
    "    (\"layer_norm.weight\", \"layer_norm/gamma\"),\n",
    "    (\"layer_norm.bias\", \"layer_norm.beta\"),\n",
    "    (\"weight\", \"kernel\"),\n",
    "    (\".\", \"/\"),\n",
    ")\n",
    "\n",
    "# fill-in PyTorch keys to ignore below\n",
    "KEYS_TO_IGNORE = []\n",
    "\n",
    "ACCEPTABLE_HF_IDS = [\"facebook/wav2vec2-base-960h\", \n",
    "                     \"facebook/wav2vec2-base\", \n",
    "                     \"facebook/wav2vec2-large-robust\", \n",
    "                     \"facebook/wav2vec2-large-xlsr-53\",\n",
    "                     \"arijitx/wav2vec2-xls-r-300m-bengali\"]\n",
    "\n",
    "PREFIX_WITH_HEAD = \"wav2vec2-ctc/\"\n",
    "SPECIAL_MAPPING_WITH_HEAD = {\n",
    "    \"wav2vec2.encoder.pos_conv_embed.conv.weight_g\": f\"{PREFIX_WITH_HEAD}wav2vec2/encoder/pos_conv_embed/conv/weight_g:0\",\n",
    "    \"wav2vec2.encoder.pos_conv_embed.conv.weight_v\": f\"{PREFIX_WITH_HEAD}wav2vec2/encoder/pos_conv_embed/conv/weight_v:0\",\n",
    "}\n",
    "\n",
    "PREFIX_WITHOUT_HEAD = \"wav2vec2/\"\n",
    "SPECIAL_MAPPING_WITHOUT_HEAD = {\n",
    "    \"encoder.pos_conv_embed.conv.weight_g\": f\"{PREFIX_WITHOUT_HEAD}encoder/pos_conv_embed/conv/weight_g:0\",\n",
    "    \"encoder.pos_conv_embed.conv.weight_v\": f\"{PREFIX_WITHOUT_HEAD}encoder/pos_conv_embed/conv/weight_v:0\",\n",
    "}\n",
    "\n",
    "\n",
    "def replace(k: str, prefix) -> str:\n",
    "    \"\"\"\n",
    "    Converts PyTorch state_dict keys to TensorFlow varible name.\n",
    "    \"\"\"\n",
    "    for hf_v, tf_v in MAPPING:\n",
    "        k = k.replace(hf_v, tf_v)\n",
    "    return prefix + k + SUFFIX\n",
    "\n",
    "\n",
    "def get_tf_pretrained_model(\n",
    "    config: Wav2Vec2Config,\n",
    "    hf_model_id: str,\n",
    "    verbose=False,\n",
    "    with_lm_head=True,\n",
    ") -> Union[Wav2Vec2ForCTC, Wav2Vec2Model]:\n",
    "    \"\"\"\n",
    "    Converts HuggingFace PyTorch weights to TensorFlow compatible weights.\n",
    "    Args:\n",
    "        config (:obj: `Wav2Vec2Config`):\n",
    "            Configuration of TF model.\n",
    "        hf_model_id (:obj: `str`):\n",
    "            model_id of HuggingFace PyTorch model.\n",
    "        with_lm_head (:obj: `bool`, default=True):\n",
    "            Whether to return Wav2Vec2ForCTC or Wav2Vec2Model\n",
    "    Returns:\n",
    "        Instance of `Wav2Vec2ForCTC` loaded with pre-trained weights.\n",
    "    \"\"\"\n",
    "    assert hf_model_id in ACCEPTABLE_HF_IDS, f\"{hf_model_id} is not acceptable\"\n",
    "\n",
    "    if with_lm_head:\n",
    "        tf_model = Wav2Vec2ForCTC(config)\n",
    "        prefix = PREFIX_WITH_HEAD\n",
    "        hf_model = transformers.Wav2Vec2ForCTC.from_pretrained(hf_model_id)\n",
    "    else:\n",
    "        tf_model = Wav2Vec2Model(config)\n",
    "        tf_model._init(input_shape=(1, 2048))\n",
    "        prefix = PREFIX_WITHOUT_HEAD\n",
    "        hf_model = transformers.Wav2Vec2Model.from_pretrained(hf_model_id)\n",
    "\n",
    "    hf_state_dict = hf_model.state_dict()\n",
    "\n",
    "    tf_variables = tf_model.variables\n",
    "    tf_variables_dict = {}\n",
    "    for v in tf_variables:\n",
    "        tf_variables_dict[v.name] = v\n",
    "\n",
    "    tf_weights = []\n",
    "    extra_keys = []\n",
    "    for k in tqdm(hf_state_dict, desc=\"hf -> tf\"):\n",
    "        if k in KEYS_TO_IGNORE:\n",
    "            continue\n",
    "\n",
    "        if k in SPECIAL_MAPPING_WITH_HEAD or k in SPECIAL_MAPPING_WITHOUT_HEAD:\n",
    "            new_k = (\n",
    "                SPECIAL_MAPPING_WITH_HEAD[k]\n",
    "                if with_lm_head\n",
    "                else SPECIAL_MAPPING_WITHOUT_HEAD[k]\n",
    "            )\n",
    "        else:\n",
    "            new_k = replace(k, prefix=prefix)\n",
    "\n",
    "        if new_k not in tf_variables_dict.keys():\n",
    "            extra_keys.append(k)\n",
    "            print(f\"SKIPPING {k}\")\n",
    "            continue\n",
    "\n",
    "        if verbose:\n",
    "            print(k, \"->\", new_k)\n",
    "\n",
    "        array = hf_state_dict[k].numpy()\n",
    "\n",
    "        # transpose the PyTorch weights for correct loading in TF-2\n",
    "        # Weights corresponding to `SPECIAL_MAPPING` are 3D array while other weights are 2D\n",
    "        # so we need to separate weights first & do special transpose on 3D weights\n",
    "        if k in SPECIAL_MAPPING_WITH_HEAD or k in SPECIAL_MAPPING_WITHOUT_HEAD:\n",
    "            array = np.transpose(array, axes=(2, 1, 0))\n",
    "        elif \"kernel\" in new_k:\n",
    "            array = np.transpose(array)\n",
    "\n",
    "        tf_weights.append((tf_variables_dict[new_k], array))\n",
    "\n",
    "    print(\"EXTRA KEYS:\\n\", extra_keys)\n",
    "\n",
    "    tf.keras.backend.batch_set_value(tf_weights)\n",
    "\n",
    "    return tf_model, hf_model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "is_robust= True \n",
    "with_lm_head=True\n",
    "model_id =\"tf-wav2vec2-xls-r-300m-bengali\"\n",
    "hf_model_id=\"arijitx/wav2vec2-xls-r-300m-bengali\"\n",
    "###########################\n",
    "config = Wav2Vec2Config() if not is_robust else RobustWav2Vec2Config()\n",
    "config.vocab_size=112    \n",
    "tf_model, hf_model = get_tf_pretrained_model(config, hf_model_id, verbose=True, with_lm_head=with_lm_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sr3PVY2aNmx9"
   },
   "source": [
    "# Data Access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PEZHa0xJLn8g"
   },
   "source": [
    "* **PER_REPLICA_BATCH_SIZE**  global batch size while training will be **8 times the PER_REPLICA_BATCH_SIZE** we provide \n",
    "\n",
    "* **REC_SIZE=256** simply means while creating the tfrecords , we stored 256 audio files with their labels in one tfrecord\n",
    "\n",
    "* for params\n",
    "```python\n",
    "PER_REPLICA_BATCH_SIZE  = 32      # this is a safe batch size \n",
    "EPOCHS                  = 50      # change this as needed .. keep the kaggle allowed TPU limit of 9 hours in mind    \n",
    "```\n",
    "* to use the full-dataset\n",
    "\n",
    "```python\n",
    "TRAIN_GCS_PATTERNS      = [os.path.join(GCS_PATH,\"voted\",\"*/*.tfrecord\"),\n",
    "                           os.path.join(GCS_PATH,\"unverified\",\"*/*.tfrecord\"),]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gxC5GZBmLn8j",
    "tags": []
   },
   "source": [
    "# Initialize TPU\n",
    "* we initialize the tpu cluster for using\n",
    "* based on number of **replicas** or devices we fix:\n",
    "    * BATCH_SIZE\n",
    "    * STEPS_PER_EPOCH\n",
    "    * and evaluation steps within an epoch (EVAL_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11626,
     "status": "ok",
     "timestamp": 1657278036958,
     "user": {
      "displayName": "kugelblitz 1729",
      "userId": "14824418471120817982"
     },
     "user_tz": -360
    },
    "id": "aoOTdXZpLn8j",
    "outputId": "e365a2b1-b3bf-4e99-9bb5-7697a70b3e68"
   },
   "outputs": [],
   "source": [
    "#----------------------------------------------------------\n",
    "# Detect hardware, return appropriate distribution strategy\n",
    "#----------------------------------------------------------\n",
    "import tensorflow as tf\n",
    "print(\"Tensorflow version \" + tf.__version__)\n",
    "\n",
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
    "    print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
    "except ValueError:\n",
    "    raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
    "\n",
    "tf.config.experimental_connect_to_cluster(tpu)\n",
    "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "strategy = tf.distribute.experimental.TPUStrategy(tpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11626,
     "status": "ok",
     "timestamp": 1657278036958,
     "user": {
      "displayName": "kugelblitz 1729",
      "userId": "14824418471120817982"
     },
     "user_tz": -360
    },
    "id": "aoOTdXZpLn8j",
    "outputId": "e365a2b1-b3bf-4e99-9bb5-7697a70b3e68"
   },
   "outputs": [],
   "source": [
    "#-------------------------------------\n",
    "# batching , strategy and steps\n",
    "#-------------------------------------\n",
    "if strategy.num_replicas_in_sync==1:\n",
    "    BATCH_SIZE = PER_REPLICA_BATCH_SIZE\n",
    "else:\n",
    "    BATCH_SIZE = PER_REPLICA_BATCH_SIZE*strategy.num_replicas_in_sync\n",
    "\n",
    "# set    \n",
    "STEPS_PER_EPOCH = (len(train_recs)*REC_SIZE)//(BATCH_SIZE)\n",
    "EVAL_STEPS      = (len(eval_recs)*REC_SIZE)//(2*BATCH_SIZE)\n",
    "print(\"Batch Size:\",BATCH_SIZE)\n",
    "print(\"Steps:\",STEPS_PER_EPOCH)\n",
    "print(\"Eval Steps:\",EVAL_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4450,
     "status": "ok",
     "timestamp": 1657278025337,
     "user": {
      "displayName": "kugelblitz 1729",
      "userId": "14824418471120817982"
     },
     "user_tz": -360
    },
    "id": "Z7fTXNgsLn8i",
    "outputId": "cd14876a-6b69-46ff-c901-859d70f457e5"
   },
   "outputs": [],
   "source": [
    "#-------------------------------\n",
    "# imports\n",
    "#-------------------------------\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import random\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "from tqdm.auto import tqdm\n",
    "from IPython.display import display,Audio\n",
    "from wav2vec2 import Wav2Vec2Config,CTCLoss\n",
    "tqdm.pandas()\n",
    "\n",
    "#--------------------------\n",
    "# GCS Paths and tfrecords\n",
    "#-------------------------\n",
    "train_recs=[]\n",
    "eval_recs =[]\n",
    "def get_tfrecs(gcs_pattern):\n",
    "    file_paths = tf.io.gfile.glob(gcs_pattern)\n",
    "    random.shuffle(file_paths)\n",
    "    print(\"found \",len(file_paths), \"tfrecords\")\n",
    "    return file_paths\n",
    "\n",
    "for gcs in TRAIN_GCS_PATTERNS:\n",
    "    print(\"Looking into gcs path:\",gcs)\n",
    "    train_recs+=get_tfrecs(gcs)\n",
    "for gcs in EVAL_GCS_PATTERNS:\n",
    "    print(gcs)\n",
    "    eval_recs+=get_tfrecs(gcs)\n",
    "\n",
    "print(\"Total Eval-recs:\",len(eval_recs))\n",
    "print(\"Total Train-recs:\",len(train_recs))\n",
    "#------------------------------------------------\n",
    "# change config\n",
    "#------------------------------------------------\n",
    "config = Wav2Vec2Config()\n",
    "config.vocab_size=len(VOCAB)+1\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bB9uvms3Ln8h"
   },
   "outputs": [],
   "source": [
    "import os \n",
    "#------------------------------\n",
    "# change able params\n",
    "#------------------------------\n",
    "TRAIN_GCS_PATTERNS      = [os.path.join(GCS_PATH,\"voted\",\"*/*.tfrecord\"),\n",
    "                           os.path.join(GCS_PATH,\"unverified\",\"*/*.tfrecord\")]\n",
    "                          \n",
    "EVAL_GCS_PATTERNS       = [os.path.join(GCS_PATH,\"eval\",\"*/*.tfrecord\")]\n",
    "\n",
    "PER_REPLICA_BATCH_SIZE  = 32      # this is a safe batch size \n",
    "EPOCHS                  = 25      # change this as needed .. keep the kaggle allowed TPU limit of 9 hours in mind    \n",
    "\n",
    "#------------------------------\n",
    "# fixed params while creating the tfrecords\n",
    "#------------------------------\n",
    "REC_SIZE=256  \n",
    "VOCAB   =[ 'pad','start','end','\\u200d',\n",
    "        ' ','!',\"'\",',','-','.',':',';','=','?','।',\n",
    "        'ঁ','ং','ঃ',\n",
    "        'অ','আ','ই','ঈ','উ','ঊ','ঋ','এ','ঐ','ও','ঔ',\n",
    "        'ক','খ','গ','ঘ','ঙ',\n",
    "        'চ','ছ','জ','ঝ','ঞ',\n",
    "        'ট','ঠ','ড','ঢ','ণ',\n",
    "        'ত','থ','দ','ধ','ন',\n",
    "        'প','ফ','ব','ভ','ম',\n",
    "        'য','র','ল',\n",
    "        'শ','ষ','স','হ',\n",
    "        'া','ি','ী','ু','ূ','ৃ','ে','ৈ','ো','ৌ','্',\n",
    "        'ৎ','ড়','ঢ়','য়',\n",
    "        '০','১','২','৩','৪','৫','৬','৭','৮','৯']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cWRDC6oALn8h"
   },
   "source": [
    "We import needed libraries here and collect the tfrecord paths that can be fed into [tf.data api](https://www.tensorflow.org/api_docs/python/tf/data/Dataset)which is the official way to use tfrecords "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-h_SBtPBSXUT"
   },
   "source": [
    "### Imports and data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCS_PATH='gs://kds-90328aa8d26e17c5bffb9a7f73013580f05a9bfecda822e30cc04946'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IdUqYVZ0Ln8k"
   },
   "source": [
    "# Data Loader \n",
    "* cfg = our data config and some constant storing\n",
    "* config=actual wave2vec2 modeling config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ydv0NrOtSLvI"
   },
   "outputs": [],
   "source": [
    "class cfg:\n",
    "    audio_shape      =  (246000,)                   # this is actually fixed for the pretrained weights we are using -- highets audio length=15 secs\n",
    "    label_shape      =  (250,)                      # this is actually fixed for the pretrained weights we are using \n",
    "    sample_rate      =  16000\n",
    "    shuffle_buffer   =  1024\n",
    "    batch_size       =  BATCH_SIZE\n",
    "    vocab_len        =  len(VOCAB)+1                # the additional vocab can account for <UNK>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r6HBseySLn8k"
   },
   "outputs": [],
   "source": [
    "#------------------------------\n",
    "# parsing tfrecords \n",
    "#------------------------------\n",
    "def normalize(x):\n",
    "    # -> (1, seqlen)\n",
    "    mean = tf.reduce_mean(x, axis=-1, keepdims=True)\n",
    "    var = tf.math.reduce_variance(x, axis=-1, keepdims=True)\n",
    "    return tf.squeeze((x - mean) / tf.sqrt(var + 1e-5))\n",
    "\n",
    "def read_raw_audio(audio):\n",
    "    wave,rate = tf.audio.decode_wav(audio, desired_channels=1, desired_samples=-1)\n",
    "    return tf.reshape(wave, shape=[-1]) \n",
    "    \n",
    "def preprocess_example(audio,label):\n",
    "    with tf.device(\"/CPU:0\"):\n",
    "        signal = normalize(read_raw_audio(audio))\n",
    "        label = tf.strings.to_number(tf.strings.split(label), out_type=tf.int32)\n",
    "        return signal,label\n",
    "\n",
    "def data_input_fn(recs): \n",
    "    '''\n",
    "      This Function generates data from gcs\n",
    "      * The parser function should look similiar now because of datasetEDA\n",
    "    '''\n",
    "    def _parser(example):   \n",
    "        feature ={  'audio' : tf.io.FixedLenFeature([],tf.string) ,\n",
    "                    'label' : tf.io.FixedLenFeature([],tf.string) \n",
    "        }    \n",
    "        example=tf.io.parse_single_example(example,feature)\n",
    "        audio,label=preprocess_example(**example)\n",
    "        return audio,label\n",
    "    # fixed code (for almost all tfrec training)\n",
    "\n",
    "    dataset = tf.data.TFRecordDataset(recs)\n",
    "    dataset = dataset.map(_parser)\n",
    "    dataset = dataset.shuffle(cfg.shuffle_buffer,reshuffle_each_iteration=True)\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.padded_batch(cfg.batch_size, padded_shapes=(cfg.audio_shape[0],cfg.label_shape[0]), padding_values=(0.0, 0))\n",
    "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.apply(tf.data.experimental.ignore_errors())\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qt44lD2ZLn8m"
   },
   "outputs": [],
   "source": [
    "train_ds=data_input_fn(train_recs)\n",
    "eval_ds =data_input_fn(eval_recs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kr2LdngtSGfy"
   },
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 15132,
     "status": "ok",
     "timestamp": 1657278064182,
     "user": {
      "displayName": "kugelblitz 1729",
      "userId": "14824418471120817982"
     },
     "user_tz": -360
    },
    "id": "_5jRxL1OLn8m",
    "outputId": "c2491bf9-4bbc-40dc-8a17-f6570b9e3fc3"
   },
   "outputs": [],
   "source": [
    "#------------------------------\n",
    "# view data\n",
    "#------------------------------\n",
    "for x,y in eval_ds.take(1):\n",
    "    signal=x[0].numpy()\n",
    "    display(Audio(data=signal, rate=cfg.sample_rate))\n",
    "    label=y[0].numpy()\n",
    "    sen=\"\".join([VOCAB[int(i)] for i in label if i > VOCAB.index(\"end\")])\n",
    "    print(\"label:\",sen)\n",
    "    print(\"input shape:\",x.shape)\n",
    "    print(\"output shape:\",y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RapqzxDCLn8n"
   },
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(cfg):\n",
    "    load_locally = tf.saved_model.LoadOptions(experimental_io_device='/job:localhost')\n",
    "    pretrained_layer = hub.KerasLayer(\"https://tfhub.dev/vasudevgupta7/wav2vec2/1\",load_options=load_locally,trainable=True)\n",
    "    inputs = tf.keras.Input(shape=cfg.audio_shape)\n",
    "    states = pretrained_layer(inputs)\n",
    "    logits= tf.keras.layers.Dense(cfg.vocab_len)(states)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=logits)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**model weights can be loaded from saved ones to continue training**\n",
    "```python\n",
    "model.load_weights(\"path to previously trained weights\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5343,
     "status": "ok",
     "timestamp": 1657278099188,
     "user": {
      "displayName": "kugelblitz 1729",
      "userId": "14824418471120817982"
     },
     "user_tz": -360
    },
    "id": "tauFmCJRLn8p",
    "outputId": "e240cfe6-d25b-4637-dbfe-92685690822f"
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    model=create_model(cfg)\n",
    "    # model.load_weights(\"model.h5\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1rRC24zKOTVK"
   },
   "source": [
    "# Training\n",
    "* some ideas to extend: \n",
    "    * use different schedulers\n",
    "    * use callbacks to track some metrics\n",
    "    * reduce learning rate on plateau, early stopping setup might need some inspection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ji6rjq9xR7ZA"
   },
   "outputs": [],
   "source": [
    "    \n",
    "# early stopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=10, \n",
    "                                                  verbose=1, \n",
    "                                                  mode = 'auto') \n",
    "lr_reducer=tf.keras.callbacks.ReduceLROnPlateau( patience=3)\n",
    "model_save=tf.keras.callbacks.ModelCheckpoint(\"model.h5\",\n",
    "                                                save_best_only=True,\n",
    "                                                save_weights_only=True,\n",
    "                                                verbose=1)\n",
    "callbacks = [lr_reducer,model_save]\n",
    "\n",
    "with strategy.scope():\n",
    "    loss_fn = CTCLoss(config, (PER_REPLICA_BATCH_SIZE,cfg.audio_shape[0]), division_factor=PER_REPLICA_BATCH_SIZE)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(5e-5),\n",
    "                  loss=loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9777524,
     "status": "ok",
     "timestamp": 1657287889381,
     "user": {
      "displayName": "kugelblitz 1729",
      "userId": "14824418471120817982"
     },
     "user_tz": -360
    },
    "id": "xsZRmDMyLn8q",
    "outputId": "4449c29f-0b2d-4461-f0e5-9ea3f7f762fc"
   },
   "outputs": [],
   "source": [
    "history=model.fit(train_ds,\n",
    "                  epochs=EPOCHS,\n",
    "                  steps_per_epoch=STEPS_PER_EPOCH,\n",
    "                  verbose=1,\n",
    "                  validation_data=eval_ds,\n",
    "                  validation_steps=EVAL_STEPS, \n",
    "                  callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zzgH25SYLn8q"
   },
   "outputs": [],
   "source": [
    "curves={}\n",
    "for key in history.history.keys():\n",
    "    curves[key]=history.history[key]\n",
    "curves=pd.DataFrame(curves)\n",
    "curves.to_csv(f\"history.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 462,
     "status": "ok",
     "timestamp": 1657293740858,
     "user": {
      "displayName": "kugelblitz 1729",
      "userId": "14824418471120817982"
     },
     "user_tz": -360
    },
    "id": "lwGtVNsnHCRF",
    "outputId": "17138be5-b3e0-4e7d-9d0c-79fe1784a689"
   },
   "outputs": [],
   "source": [
    "curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U89_QPqmaxBN"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
