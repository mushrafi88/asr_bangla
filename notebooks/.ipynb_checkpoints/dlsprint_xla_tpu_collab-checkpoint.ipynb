{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35b86184-f9ec-4b06-b2e6-85a2f70b9c0d",
   "metadata": {},
   "source": [
    "# Package and dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3cdfc1-bc84-48b0-9b34-4a4c58f1f0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1aef150-2717-4c29-b89c-d242a1da6b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"facebook/wav2vec2-large-xlsr-53\"\n",
    "batch_size = 16\n",
    "train_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a319e417-c309-4680-856e-642eae5d1299",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchaudio==0.11\n",
    "!pip install seaborn\n",
    "!pip install kaggle\n",
    "!pip install -U transformers\n",
    "!pip install datasets\n",
    "!pip install fsspec==2021.5.0\n",
    "!pip install jiwer==2.2.0\n",
    "!pip install pydub\n",
    "!pip install librosa\n",
    "!pip install bnunicodenormalizer\n",
    "!pip install pandarallel\n",
    "!pip install accelerate \n",
    "!pip install nvidia-ml-py3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76331f6-ab40-4dd5-9870-d2b4730ae326",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install cloud-tpu-client==0.10 torch==1.11.0 https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-1.11-cp37-cp37m-linux_x86_64.whl\n",
    "!pip install git+https://github.com/huggingface/accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3815591a-3b14-4e70-9762-2725afc0157b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/root/.kaggle’: Permission denied\n",
      "mv: cannot stat '/root/.kaggle/kaggle.json': Permission denied\n"
     ]
    }
   ],
   "source": [
    "# unhash for collab only\n",
    "\n",
    "!mkdir /root/.kaggle\n",
    "\n",
    "\n",
    "import json\n",
    "dictionary ={\n",
    "\"username\":\"mushrafimunim\",\n",
    "\"key\":\"e5c337a189ee0a5c867ff83c21df4d2a\"\n",
    "}\n",
    "  \n",
    "json_object = json.dumps(dictionary, indent = 4)\n",
    "  \n",
    "with open(\"kaggle.json\", \"w\") as outfile:\n",
    "    outfile.write(json_object)\n",
    "\n",
    "%mv kaggle.json /root/.kaggle/kaggle.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf81016d-3f18-4174-9161-4c940c792c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "from IPython import display as ipd\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n",
    "import seaborn as sns\n",
    "#system files\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import glob\n",
    "\n",
    "import zipfile\n",
    "import shutil\n",
    "import gc\n",
    "from pydub import AudioSegment\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "#transformers\n",
    "from transformers import (AdamW,\n",
    "                          AutoTokenizer,\n",
    "                          AutoFeatureExtractor,\n",
    "                          AutoConfig,\n",
    "                          AutoModel,\n",
    "                          Wav2Vec2CTCTokenizer,\n",
    "                          Wav2Vec2ForCTC,\n",
    "                          Wav2Vec2Processor,\n",
    "                          Trainer,\n",
    "                          TrainingArguments,\n",
    "                          Wav2Vec2FeatureExtractor,\n",
    "                          get_linear_schedule_with_warmup,\n",
    "                          set_seed)\n",
    "# PyTorch \n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda import amp\n",
    "import torch.nn.functional as F\n",
    "import torchaudio.functional as FT\n",
    "import torchaudio.transforms as TT\n",
    "\n",
    "\n",
    "#sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset, load_metric,Dataset,concatenate_datasets,set_caching_enabled, ClassLabel\n",
    "\n",
    "#import librosa\n",
    "\n",
    "#normalization\n",
    "from pandarallel import pandarallel\n",
    "from bnunicodenormalizer import Normalizer \n",
    "pandarallel.initialize(progress_bar=True,nb_workers=8)\n",
    "tqdm.pandas()\n",
    "bnorm=Normalizer()\n",
    "\n",
    "# Set environment variables\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#accelerator\n",
    "from accelerate import Accelerator, DistributedType\n",
    "\n",
    "from datasets import load_dataset, load_metric \n",
    "import datasets\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2e4540-7629-4e5f-8e94-7e955e3cba13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#unhash for collab only\n",
    "\n",
    "!kaggle competitions download -c dlsprint\n",
    "\n",
    "!unzip dlsprint.zip\n",
    "\n",
    "!rm -rf dlsprint.zip\n",
    "!git clone https://gitlab.com/mushrafi88/dlsprint.git\n",
    "!cp /content/dlsprint/vocab.json /content/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb95a2a2-802e-45df-a4c8-8e4c8be9b833",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('dlsprint/df_train_sen+duration.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e19b788-3ef3-4978-ab01-37ca7c18f788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_files/common_voice_bn_30991326.mp3</td>\n",
       "      <td>বাবা সত্যেন ঘোষ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_files/common_voice_bn_30991432.mp3</td>\n",
       "      <td>আপনি খুব একটা কথা বলার লোক নন তাই না</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_files/common_voice_bn_30991478.mp3</td>\n",
       "      <td>আপনি খুব একটা কথা বলার লোক নন তাই না</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_files/common_voice_bn_30991480.mp3</td>\n",
       "      <td>তার সাম্রাজ্য ছিল বিশ্বজুড়ে</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_files/common_voice_bn_30991488.mp3</td>\n",
       "      <td>আক্রমণাত্মক ব্যাটিং ও দ্রুত রান সংগ্রাহক হিসেব...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107860</th>\n",
       "      <td>validation_files/common_voice_bn_31520874.mp3</td>\n",
       "      <td>ব্রিস্টল নতুন বিশ্ব অনুসন্ধানের যাত্রা শুরু কর...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107861</th>\n",
       "      <td>validation_files/common_voice_bn_31541319.mp3</td>\n",
       "      <td>পাণ্ডুর দুই স্ত্রী</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107862</th>\n",
       "      <td>validation_files/common_voice_bn_31541568.mp3</td>\n",
       "      <td>তাকে ধরে ব্রিটিশ পুলিশ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107863</th>\n",
       "      <td>validation_files/common_voice_bn_30998752.mp3</td>\n",
       "      <td>তুমি ওকে ভালো করে বুঝিয়ে দিও</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107864</th>\n",
       "      <td>validation_files/common_voice_bn_30998756.mp3</td>\n",
       "      <td>তবে রোগাক্রান্ত পোষা বিড়ালের মাধ্যমে এটি বেশি ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>107865 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 path  \\\n",
       "0            train_files/common_voice_bn_30991326.mp3   \n",
       "1            train_files/common_voice_bn_30991432.mp3   \n",
       "2            train_files/common_voice_bn_30991478.mp3   \n",
       "3            train_files/common_voice_bn_30991480.mp3   \n",
       "4            train_files/common_voice_bn_30991488.mp3   \n",
       "...                                               ...   \n",
       "107860  validation_files/common_voice_bn_31520874.mp3   \n",
       "107861  validation_files/common_voice_bn_31541319.mp3   \n",
       "107862  validation_files/common_voice_bn_31541568.mp3   \n",
       "107863  validation_files/common_voice_bn_30998752.mp3   \n",
       "107864  validation_files/common_voice_bn_30998756.mp3   \n",
       "\n",
       "                                                 sentence  \n",
       "0                                         বাবা সত্যেন ঘোষ  \n",
       "1                    আপনি খুব একটা কথা বলার লোক নন তাই না  \n",
       "2                    আপনি খুব একটা কথা বলার লোক নন তাই না  \n",
       "3                             তার সাম্রাজ্য ছিল বিশ্বজুড়ে  \n",
       "4       আক্রমণাত্মক ব্যাটিং ও দ্রুত রান সংগ্রাহক হিসেব...  \n",
       "...                                                   ...  \n",
       "107860  ব্রিস্টল নতুন বিশ্ব অনুসন্ধানের যাত্রা শুরু কর...  \n",
       "107861                                 পাণ্ডুর দুই স্ত্রী  \n",
       "107862                             তাকে ধরে ব্রিটিশ পুলিশ  \n",
       "107863                       তুমি ওকে ভালো করে বুঝিয়ে দিও  \n",
       "107864  তবে রোগাক্রান্ত পোষা বিড়ালের মাধ্যমে এটি বেশি ...  \n",
       "\n",
       "[107865 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de190159-3cb1-4a69-af5c-050b40956b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = Dataset.from_pandas(df_train.sample(train_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3879c3d6-facd-43e4-a4ec-ecda7a0abf08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['path', 'sentence', '__index_level_0__'],\n",
       "    num_rows: 500\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3598a2c-d9c6-4320-b678-39bf891a55e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.save_to_disk(\"dlsprint/train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b94b456-33f5-437a-804c-821c79c99280",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = Dataset.load_from_disk(\"dlsprint/train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6748ff6-7e22-4532-aa51-da6fd7a26669",
   "metadata": {},
   "outputs": [],
   "source": [
    "resamplers = {  \n",
    "    48000: torchaudio.transforms.Resample(48000, 16000),\n",
    "    44100: torchaudio.transforms.Resample(44100, 16000),\n",
    "    32000: torchaudio.transforms.Resample(32000, 16000),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9854f586-58f0-47df-b04b-8d56e749fee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speech_file_to_array_torch(batch):\n",
    "    speech_array, sampling_rate = torchaudio.load(batch[\"path\"])\n",
    "    batch[\"speech\"] = resamplers[sampling_rate](speech_array).squeeze().numpy()\n",
    "    batch[\"speech\"] = np.trim_zeros(batch[\"speech\"])\n",
    "    batch[\"sampling_rate\"] = sampling_rate\n",
    "    batch[\"target_text\"] = batch[\"sentence\"]\n",
    "    return batch\n",
    "def speech_file_to_array_submission_torch(batch):\n",
    "    speech_array, sampling_rate = torchaudio.load(batch[\"path\"])\n",
    "    batch[\"speech\"] = resamplers[sampling_rate](speech_array).squeeze().numpy()\n",
    "    batch[\"speech\"] = np.trim_zeros(batch[\"speech\"])\n",
    "    batch[\"sampling_rate\"] = sampling_rate\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30fcb3fb-7e7e-4b12-9124-41ba50414b0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "578"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afef80be-9cad-4713-a68f-54d16eb5f70b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['path', 'sentence', '__index_level_0__'],\n",
       "    num_rows: 500\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c34bac8-f23b-4afd-b24c-60e1f5e3962a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function speech_file_to_array_torch at 0x7ff2f32f5fc0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ecec407445d46218682ca44468a0256",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = train.map(speech_file_to_array_torch, remove_columns=train.column_names,batch_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69a54f48-2cb2-4342-8e4b-b38917d14c62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'speech': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None),\n",
       " 'sampling_rate': Value(dtype='int64', id=None),\n",
       " 'target_text': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45c8c461-725c-4a4b-9af5-643b3c9e4be1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.cleanup_cache_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58e3d6e3-0978-4219-9b6a-9099e9fe171f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7421968-d695-4c6d-9b1c-68265fe8b91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.set_format(\"numpy\", columns=[\"speech\",\"sampling_rate\"], output_all_columns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "043f928e-c002-4229-8c97-6ddd589e73db",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(model_checkpoint)\n",
    "tokenizer_type = config.model_type if config.tokenizer_class is None else None\n",
    "config = config if config.tokenizer_class is not None else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3729a29a-2871-4fb9-9aa0-bc67716b5c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "  \"./\",\n",
    "  config=config,\n",
    "  tokenizer_type=tokenizer_type,\n",
    "  unk_token=\"[UNK]\",\n",
    "  pad_token=\"[PAD]\",\n",
    "  word_delimiter_token=\"|\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e2a77e4f-7408-4fb5-aaa6-9aa4ff4d2fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = AutoFeatureExtractor.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c7115272-af90-4b38-81cf-5953686f828f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wav2Vec2FeatureExtractor {\n",
       "  \"do_normalize\": true,\n",
       "  \"feature_extractor_type\": \"Wav2Vec2FeatureExtractor\",\n",
       "  \"feature_size\": 1,\n",
       "  \"padding_side\": \"right\",\n",
       "  \"padding_value\": 0,\n",
       "  \"return_attention_mask\": true,\n",
       "  \"sampling_rate\": 16000\n",
       "}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "263b8057-9263-4ead-9ed7-fb0de879ac81",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d11aaa6d-f818-4bea-b0a9-a94c948a1080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wav2Vec2Processor:\n",
       "- feature_extractor: Wav2Vec2FeatureExtractor {\n",
       "  \"do_normalize\": true,\n",
       "  \"feature_extractor_type\": \"Wav2Vec2FeatureExtractor\",\n",
       "  \"feature_size\": 1,\n",
       "  \"padding_side\": \"right\",\n",
       "  \"padding_value\": 0,\n",
       "  \"return_attention_mask\": true,\n",
       "  \"sampling_rate\": 16000\n",
       "}\n",
       "\n",
       "- tokenizer: PreTrainedTokenizer(name_or_path='./', vocab_size=64, model_max_len=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '[UNK]', 'pad_token': '[PAD]', 'additional_special_tokens': [AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True)]})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "430324d4-b742-4726-81f2-75641d028092",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.save_pretrained(\"model/wav2vec2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "97b107e3-209f-4cf3-b134-d8d09f1f6a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_array_size(batch):\n",
    "    batch['input_size'] = np.size(batch['speech'])\n",
    "    batch['labels_size'] = len(batch['target_text'])\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eaa567c9-c177-4051-ba0d-ed7701b39a35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71e5c380c417482fbf029009f38e85bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = train.map(get_array_size)\n",
    "#test = test.map(get_array_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b41cb2b4-33d3-43eb-ade9-c4258c24997a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'speech': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None),\n",
       " 'sampling_rate': Value(dtype='int64', id=None),\n",
       " 'target_text': Value(dtype='string', id=None),\n",
       " 'input_size': Value(dtype='int64', id=None),\n",
       " 'labels_size': Value(dtype='int64', id=None)}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "28c1845f-2be1-46a7-888c-56167c9ff611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'speech': array([-3.5619417e-11,  3.2330399e-10, -1.0085299e-09, ...,\n",
       "         2.0057014e-03,  2.1096386e-03,  2.1697129e-03], dtype=float32),\n",
       " 'sampling_rate': 32000,\n",
       " 'input_size': 50058,\n",
       " 'labels_size': 32,\n",
       " 'target_text': 'চাইলে গ্রিল করেও নেওয়া যেতে পারে'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "183a2a5b-c522-4d43-a135-e79d41165482",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features_max_train=train['input_size'].max()\n",
    "labels_features_max_train=train['labels_size'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "064898be-f3a4-423b-b391-6fd948b23d73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_features_max_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "38486b06-476f-46e5-b078-becc3f8767e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "l=processor(train['speech'][0],sampling_rate=16000,padding='max_length',max_length=input_features_max_train,return_tensor='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4c166de3-bcb1-4553-ba7d-1fdf99ccbd89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_values': [array([-0.00021754, -0.00021753, -0.00021754, ...,  0.        ,\n",
       "        0.        ,  0.        ], dtype=float32)], 'attention_mask': [array([1, 1, 1, ..., 0, 0, 0], dtype=int32)]}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "11b10841-5430-4641-ac24-5ffb76024050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [54, 50, 17, 44, 32, 49, 16, 23, 22, 11, 44, 49, 45, 22, 32, 48, 49, 61, 32, 48, 27, 50, 49, 14, 32, 33, 32, 49, 58, 50, 22, 32, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with processor.as_target_processor():\n",
    "    l=processor(train[\"target_text\"][0],padding='max_length',max_length=labels_features_max_train)   \n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5310df3e-c45f-4c13-a331-50b74e4df15f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.size(l.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "896b1d19-98f8-44a0-93a6-638446dd9103",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    batch[\"input_values\"] = processor(batch[\"speech\"], sampling_rate=16000,padding='max_length',max_length=input_features_max_train).input_values[0]\n",
    "    batch[\"attention_mask\"] = processor(batch[\"speech\"], sampling_rate=16000,padding='max_length',max_length=input_features_max_train).attention_mask[0]\n",
    "    with processor.as_target_processor():\n",
    "        batch[\"labels\"] = processor(batch[\"target_text\"],padding='max_length',max_length=labels_features_max_train).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb42bc9-a3c8-40ba-8bfd-437245d71db7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "53b48c92-94af-49d3-9853-c4c0c3b26714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "54d2ba41-a6a0-4d3d-afa7-98c83869f2c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a07859cbf47849d8afaaf6a9977707a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train=train.map(prepare_dataset, remove_columns=train.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "93bf5804-8c21-4be0-b7ef-32f6640beef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'speech': array([-3.5619417e-11,  3.2330399e-10, -1.0085299e-09, ...,\n",
       "         2.0057014e-03,  2.1096386e-03,  2.1697129e-03], dtype=float32),\n",
       " 'sampling_rate': 32000,\n",
       " 'input_size': 50058,\n",
       " 'labels_size': 32,\n",
       " 'input_values': array([-0.00021754, -0.00021753, -0.00021754, ...,  0.        ,\n",
       "         0.        ,  0.        ], dtype=float32),\n",
       " 'attention_mask': array([1, 1, 1, ..., 0, 0, 0], dtype=int32),\n",
       " 'labels': array([54, 50, 17, 44, 32, 49, 16, 23, 22, 11, 44, 49, 45, 22, 32, 48, 49,\n",
       "        61, 32, 48, 27, 50, 49, 14, 32, 33, 32, 49, 58, 50, 22, 32, 63, 63,\n",
       "        63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63,\n",
       "        63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63,\n",
       "        63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63,\n",
       "        63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63,\n",
       "        63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63,\n",
       "        63, 63]),\n",
       " 'target_text': 'চাইলে গ্রিল করেও নেওয়া যেতে পারে'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c0652de8-959d-43a3-bc0f-e8382b708243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.cleanup_cache_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fb57a91b-5251-4ff7-8135-5775e4ea43b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6c6a4402-9c54-4ef1-98cf-463626fc71d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'speech': array([ 7.3093590e-14,  1.3751280e-11, -2.1051676e-10, ...,\n",
       "        -2.2519783e-03, -2.4381133e-03, -3.0806779e-03], dtype=float32),\n",
       " 'sampling_rate': 32000,\n",
       " 'input_size': 77133,\n",
       " 'labels_size': 57,\n",
       " 'input_values': array([0.00029655, 0.00029656, 0.00029655, ..., 0.        , 0.        ,\n",
       "        0.        ], dtype=float32),\n",
       " 'attention_mask': array([1, 1, 1, ..., 0, 0, 0], dtype=int32),\n",
       " 'labels': array([15, 22, 49, 47,  4, 23, 14, 32, 49,  0, 44, 23, 44, 32, 29, 14, 36,\n",
       "        16, 23, 14, 49, 28, 54, 23,  5, 32, 49, 53, 23, 22, 43, 44, 19, 23,\n",
       "        45, 50, 49, 61, 23, 14, 50, 53, 61, 50, 44, 49, 58, 50, 22, 23, 44,\n",
       "        50, 47, 32, 61, 23,  1, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63,\n",
       "        63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63,\n",
       "        63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63,\n",
       "        63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63,\n",
       "        63, 63]),\n",
       " 'target_text': 'এর মধ্যে উল্লেখযোগ্য হচ্ছে শ্রীলঙ্কা ন্যাশনাল পার্লামেন্ট'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b6b02c77-672f-474c-9348-376ed039eed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'speech': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None),\n",
       " 'sampling_rate': Value(dtype='int64', id=None),\n",
       " 'target_text': Value(dtype='string', id=None),\n",
       " 'input_size': Value(dtype='int64', id=None),\n",
       " 'labels_size': Value(dtype='int64', id=None),\n",
       " 'input_values': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None),\n",
       " 'attention_mask': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None),\n",
       " 'labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c9b24f8b-ad06-4c43-9b8b-a82fc6085acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=train.remove_columns(['speech','sampling_rate','target_text','input_size','labels_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b45fd3a2-6d25-4bf7-9ff6-aca695e2a155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_values': array([-0.00021754, -0.00021753, -0.00021754, ...,  0.        ,\n",
       "         0.        ,  0.        ], dtype=float32),\n",
       " 'attention_mask': array([1, 1, 1, ..., 0, 0, 0], dtype=int32),\n",
       " 'labels': array([54, 50, 17, 44, 32, 49, 16, 23, 22, 11, 44, 49, 45, 22, 32, 48, 49,\n",
       "        61, 32, 48, 27, 50, 49, 14, 32, 33, 32, 49, 58, 50, 22, 32, 63, 63,\n",
       "        63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63,\n",
       "        63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63,\n",
       "        63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63,\n",
       "        63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63,\n",
       "        63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63,\n",
       "        63, 63])}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d67c7274-50e7-4cc0-bd4f-af5da305f9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "63fb3efd-4023-4277-b4b0-87b763289d0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_values': tensor([-0.0002, -0.0002, -0.0002,  ...,  0.0000,  0.0000,  0.0000]),\n",
       " 'attention_mask': tensor([1, 1, 1,  ..., 0, 0, 0]),\n",
       " 'labels': tensor([54, 50, 17, 44, 32, 49, 16, 23, 22, 11, 44, 49, 45, 22, 32, 48, 49, 61,\n",
       "         32, 48, 27, 50, 49, 14, 32, 33, 32, 49, 58, 50, 22, 32, 63, 63, 63, 63,\n",
       "         63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63,\n",
       "         63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63,\n",
       "         63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63,\n",
       "         63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63,\n",
       "         63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63])}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "fb339ff9-3599-40d8-bb0a-34634ac72c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test=train.train_test_split(test_size=0.1, shuffle=True)\n",
    "train = train_test['train']\n",
    "test = train_test['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "49459df4-d595-4022-bad5-501999f358e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_values', 'attention_mask', 'labels'],\n",
       "    num_rows: 450\n",
       "})"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "dd3bfc3b-ff1a-44ee-8790-cfed0516eef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_values', 'attention_mask', 'labels'],\n",
       "    num_rows: 50\n",
       "})"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "48d3dc6a-7b49-45e7-bd0d-cde0879e9db7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_values': tensor([0.0002, 0.0002, 0.0002,  ..., 0.0000, 0.0000, 0.0000]),\n",
       " 'attention_mask': tensor([1, 1, 1,  ..., 0, 0, 0]),\n",
       " 'labels': tensor([33, 11, 61, 11, 49, 21, 23, 14, 50, 54, 32, 44, 22, 49, 39, 51, 49, 44,\n",
       "         35, 49,  3, 11, 16, 23, 22, 11, 49, 39, 22, 23, 41, 61, 49, 45, 22, 32,\n",
       "          5, 32, 61, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63,\n",
       "         63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63,\n",
       "         63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63,\n",
       "         63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63,\n",
       "         63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63,\n",
       "         63, 63, 63, 63, 63, 63, 63, 63, 63])}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f9cf55bb-3abd-4ef2-b04a-c19f73f5c7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3db32c4c-bd45-4395-b833-a898dec8fdaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_values': array([ 0.00104027,  0.00104027,  0.00104027, ..., -0.00108207,\n",
       "         0.00012564,  0.00175962], dtype=float32),\n",
       " 'labels': array([ 3, 50, 61, 28, 50, 33, 32, 49, 21, 23, 14, 50,  1, 11, 34, 27, 32,\n",
       "        22, 49, 58, 50, 53, 50, 58, 50, 53, 11, 49, 47, 11,  3, 11, 27, 50,\n",
       "        47, 49, 51, 50, 35, 23,  1, 49, 21, 36, 44, 11, 34, 49, 45, 22, 32,\n",
       "        49,  8, 50, 45, 32, 61, 49, 33, 11, 61, 11])}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0a5ea25c-4e02-4637-b29e-dd3e916caa2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features = [{\"input_values\": row[\"input_values\"]} for row in train]\n",
    "label_features = [{\"input_ids\": row[\"labels\"]} for row in train]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7b5399db-855c-47bf-853a-d9fffc18a3e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(input_features[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b0f6e758-1c4b-4b87-ac24-99da7da1b68a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input_values': array([ 1.6418668e-04,  1.6419019e-04,  1.6417707e-04, ...,\n",
       "         -2.3032914e-04, -4.1316694e-06,  1.8859381e-04], dtype=float32)},\n",
       " {'input_values': array([0.00052973, 0.00052973, 0.00052973, ..., 0.00182272, 0.00191038,\n",
       "         0.00314709], dtype=float32)},\n",
       " {'input_values': array([ 0.0012399 ,  0.0012399 ,  0.00123987, ..., -0.47131214,\n",
       "         -0.37768555, -0.33655354], dtype=float32)},\n",
       " {'input_values': array([ 4.9831095e-04,  4.9831346e-04,  4.9830397e-04, ...,\n",
       "         -1.8511198e-03, -5.9825712e-04,  9.9259472e-05], dtype=float32)},\n",
       " {'input_values': array([0.00033698, 0.00033698, 0.00033697, ..., 0.02045033, 0.01980539,\n",
       "         0.02215709], dtype=float32)}]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_features[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2ff154e4-3303-4c86-a72d-33d116341e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "taku=processor.pad(input_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "dae2402c-b124-4524-b72a-b0bbc895e282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.feature_extraction_utils.BatchFeature"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(taku)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3dcb30d1-aed1-498f-935d-950255697459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_values': array([[ 1.6418668e-04,  1.6419019e-04,  1.6417707e-04, ...,\n",
       "         0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "       [ 5.2972970e-04,  5.2972918e-04,  5.2973075e-04, ...,\n",
       "         0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "       [ 1.2398950e-03,  1.2399033e-03,  1.2398731e-03, ...,\n",
       "         0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "       ...,\n",
       "       [-6.9429829e-05, -6.9430032e-05, -6.9429094e-05, ...,\n",
       "         0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "       [ 1.4050736e-03,  1.4050790e-03,  1.4050585e-03, ...,\n",
       "         0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "       [ 3.5078972e-04,  3.5079027e-04,  3.5078864e-04, ...,\n",
       "         0.0000000e+00,  0.0000000e+00,  0.0000000e+00]], dtype=float32), 'attention_mask': array([[1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0]], dtype=int32)}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "97cee818-b7d0-4fa9-b2c2-9cc98d597f11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taku['attention_mask'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d1685d8b-f93b-4f9e-8c74-5de2e9cdcad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "taku=Dataset.from_dict(taku)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be19d4b-3866-427c-b30e-ca4774d6e49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "taku[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b041ab4f-edad-43e4-b961-2b39d263845a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with processor.as_target_processor():\n",
    "            labels_batch = processor.pad(label_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5512bbe2-3074-4cb6-89b0-479ce84024c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_batch['attention_mask'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "86babdd1-effe-41f9-84ce-9aca178c01c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding2:\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = None\n",
    "    max_length_labels: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "    def __call__(self, df: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lenghts and need\n",
    "        input_features = [{\"input_values\": row[\"input_values\"]} for row in df]\n",
    "        label_features = [{\"input_ids\": row[\"labels\"]} for row in df]\n",
    "        \n",
    "        input_features=input_features[:2]\n",
    "        label_features=label_features[:2]\n",
    "        \n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding='max_length',\n",
    "            max_length=input_features_max_train,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding='max_length',\n",
    "                max_length=labels_features_max_train,\n",
    "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "        print(\"this is batch\")\n",
    "        print(batch)\n",
    "        print(\"this is labels\")\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "        print(labels)\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266b3667-03bc-4d26-8f9f-0763495afb48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d3835d51-5312-453d-a08e-12df54e7a64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = None\n",
    "    max_length_labels: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "    def __call__(self, df: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lenghts and need\n",
    "        input_features = [{\"input_values\": row[\"input_values\"]} for row in df]\n",
    "        label_features = [{\"input_ids\": row[\"labels\"]} for row in df]\n",
    "        \n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding='max_length',\n",
    "            max_length=input_features_max_train,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding='max_length',\n",
    "                max_length=labels_features_max_train,\n",
    "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"]#.masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bf6a8c36-51cb-48c9-a8e6-f0571613134d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorCTCWithPadding2(processor=processor, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6a3b84b4-d0d2-4aab-a3c7-5aa1231a3650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is batch\n",
      "{'input_values': tensor([[0.0010, 0.0010, 0.0010,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0015, 0.0015, 0.0015,  ..., 0.0000, 0.0000, 0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], dtype=torch.int32)}\n",
      "this is labels\n",
      "tensor([[   3,   50,   61,   28,   50,   33,   32,   49,   21,   23,   14,   50,\n",
      "            1,   11,   34,   27,   32,   22,   49,   58,   50,   53,   50,   58,\n",
      "           50,   53,   11,   49,   47,   11,    3,   11,   27,   50,   47,   49,\n",
      "           51,   50,   35,   23,    1,   49,   21,   36,   44,   11,   34,   49,\n",
      "           45,   22,   32,   49,    8,   50,   45,   32,   61,   49,   33,   11,\n",
      "           61,   11, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100],\n",
      "        [  33,   50,   22,   49,   35,   23,    8,   44,   50,   20,   11,   38,\n",
      "           11,   45,   23,   33,   49,   28,   61,   49,   22,   41,   50,   22,\n",
      "           49,   61,   50,   17,    1, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_values': tensor([[0.0010, 0.0010, 0.0010,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0015, 0.0015, 0.0015,  ..., 0.0000, 0.0000, 0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]], dtype=torch.int32), 'labels': tensor([[   3,   50,   61,   28,   50,   33,   32,   49,   21,   23,   14,   50,\n",
       "            1,   11,   34,   27,   32,   22,   49,   58,   50,   53,   50,   58,\n",
       "           50,   53,   11,   49,   47,   11,    3,   11,   27,   50,   47,   49,\n",
       "           51,   50,   35,   23,    1,   49,   21,   36,   44,   11,   34,   49,\n",
       "           45,   22,   32,   49,    8,   50,   45,   32,   61,   49,   33,   11,\n",
       "           61,   11, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100],\n",
       "        [  33,   50,   22,   49,   35,   23,    8,   44,   50,   20,   11,   38,\n",
       "           11,   45,   23,   33,   49,   28,   61,   49,   22,   41,   50,   22,\n",
       "           49,   61,   50,   17,    1, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100]])}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a3f30a97-ba75-48cd-b8a8-47d18a8e37ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_values', 'attention_mask', 'labels'])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8b932ae9-24df-4a56-9393-9e2499a923d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([94991])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l['attention_mask'][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "6bccb269-9947-494a-a5fa-634a73867fea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['speech', 'sampling_rate', 'target_text', 'input_size', 'labels_size'],\n",
       "    num_rows: 500\n",
       "})"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1a51676e-de2b-45af-9709-c65f7460b59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(train_batch_size=8):\n",
    "    train_dataloader = DataLoader(\n",
    "        dataset=train, shuffle=False, batch_size=train_batch_size,drop_last=True)\n",
    "    return train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bb828e9e-d34a-450a-93d6-d30463b0975a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = create_dataloaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a0c6fb91-11cb-4be6-83b8-a93f790ba695",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(train_dataloader,'train_dataloader.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "132a6218-59b3-43bb-b87a-48ebfe201a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader=torch.load('train_dataloader.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4a05a985-d9ca-494e-af91-72fdcb3f42c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f7704be6200>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(eval_dataloader,'eval_dataloader.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949ada64-ac41-4722-8896-89c32c6ada71",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataloader=torch.load('eval_dataloader')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78baa4a3-baf6-448d-8f3f-bc7943be287e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_values': torch.Size([8, 94998]), 'attention_mask': torch.Size([8, 94998]), 'labels': torch.Size([8, 121])}\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    print({k: v.shape for k, v in batch.items()})\n",
    "    outputs = model(**batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b6090a62-86b6-4454-98d3-a93ac63f3d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-large-xlsr-53 were not used when initializing Wav2Vec2ForCTC: ['project_q.weight', 'quantizer.weight_proj.weight', 'project_q.bias', 'quantizer.codevectors', 'project_hid.bias', 'quantizer.weight_proj.bias', 'project_hid.weight']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-xlsr-53 and are newly initialized: ['lm_head.bias', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCTC\n",
    "\n",
    "model = AutoModelForCTC.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    attention_dropout=0.1,\n",
    "    hidden_dropout=0.1,\n",
    "    feat_proj_dropout=0.0,\n",
    "    mask_time_prob=0.05,\n",
    "    layerdrop=0.1,\n",
    "    ctc_loss_reduction=\"mean\", \n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "    vocab_size=len(processor.tokenizer)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fb7f1fd9-1566-45e7-b9e2-3917a8d4f6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "if hasattr(model, \"freeze_feature_extractor\"):\n",
    "    model.freeze_feature_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaae949f-16dc-4ed5-8ebb-723f2b6cb372",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"learning_rate\": 0.2,\n",
    "    \"num_epochs\": 5,\n",
    "    \"train_batch_size\": 8, # Actual batch size will this x 8\n",
    "    \"eval_batch_size\": 8, # Actual batch size will this x 8\n",
    "    \"seed\": 42,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c22c758-efad-474a-90fa-3bbeafc68cc3",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55042d46-b4a9-47d1-8416-04177c9f1827",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_function(model):\n",
    "    # Initialize accelerator\n",
    "    accelerator = Accelerator()\n",
    "\n",
    "    if accelerator.is_main_process:\n",
    "        datasets.utils.logging.set_verbosity_warning()\n",
    "        transformers.utils.logging.set_verbosity_info()\n",
    "    else:\n",
    "        datasets.utils.logging.set_verbosity_error()\n",
    "        transformers.utils.logging.set_verbosity_error()\n",
    "\n",
    "    # The seed need to be set before we instantiate the model, as it will determine the random head.\n",
    "    set_seed(hyperparameters[\"seed\"])\n",
    "\n",
    "    # Instantiate optimizer\n",
    "    optimizer = AdamW(params=model.parameters(), lr=hyperparameters[\"learning_rate\"])\n",
    "\n",
    "    # Prepare everything\n",
    "    model, optimizer, train_dataloader,  = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader )\n",
    "    \n",
    "    num_epochs = hyperparameters[\"num_epochs\"]\n",
    "    # Instantiate learning rate scheduler after preparing the training dataloader as the prepare method\n",
    "    # may change its length.\n",
    "    lr_scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=100,\n",
    "        num_training_steps=len(train_dataloader) * num_epochs,\n",
    "    )\n",
    "\n",
    "    # Instantiate a progress bar to keep track of training. Note that we only enable it on the main\n",
    "    # process to avoid having 8 progress bars.\n",
    "    progress_bar = tqdm(range(num_epochs * len(train_dataloader)), disable=not accelerator.is_main_process)\n",
    "    # Now we train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ebb7b7-018f-450b-82c8-9525bb4de323",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import notebook_launcher\n",
    "\n",
    "notebook_launcher(training_function, (model,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8612cfd-be88-4ef0-ab0d-ec7f86922f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for collab only\n",
    "'''\n",
    "!cp -R /content/wav2vec2_bn /content/drive/MyDrive/buet_cse_fest_dlsprint\n",
    "from google.colab import drive\n",
    "drive.flush_and_unmount()\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
