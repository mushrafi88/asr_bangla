{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "punctuation_adder_working.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LU2xTqBJVbAW"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/xashru/punctuation-restoration.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd punctuation-restoration/"
      ],
      "metadata": {
        "id": "QJywTceQVelJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip download -r requirements.txt"
      ],
      "metadata": {
        "id": "j0MCCb-YdaaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install gdown "
      ],
      "metadata": {
        "id": "4F6u0mt8Vz1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/punctuation-restoration/punctuation_adder\n",
        "!gdown --id --folder 1PsmJvIa0MwUdryAj2pholK3_kh98nWuc"
      ],
      "metadata": {
        "id": "DiqrFpXkd4wp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import *\n",
        "\n",
        "# special tokens indices in different models available in transformers\n",
        "TOKEN_IDX = {\n",
        "    'bert': {\n",
        "        'START_SEQ': 101,\n",
        "        'PAD': 0,\n",
        "        'END_SEQ': 102,\n",
        "        'UNK': 100\n",
        "    },\n",
        "    'xlm': {\n",
        "        'START_SEQ': 0,\n",
        "        'PAD': 2,\n",
        "        'END_SEQ': 1,\n",
        "        'UNK': 3\n",
        "    },\n",
        "    'roberta': {\n",
        "        'START_SEQ': 0,\n",
        "        'PAD': 1,\n",
        "        'END_SEQ': 2,\n",
        "        'UNK': 3\n",
        "    },\n",
        "    'albert': {\n",
        "        'START_SEQ': 2,\n",
        "        'PAD': 0,\n",
        "        'END_SEQ': 3,\n",
        "        'UNK': 1\n",
        "    },\n",
        "}\n",
        "\n",
        "# 'O' -> No punctuation\n",
        "punctuation_dict = {'O': 0, 'COMMA': 1, 'PERIOD': 2, 'QUESTION': 3}\n",
        "\n",
        "\n",
        "# pretrained model name: (model class, model tokenizer, output dimension, token style)\n",
        "MODELS = {\n",
        "    'xlm-roberta-large': (XLMRobertaModel, XLMRobertaTokenizer, 1024, 'roberta')\n",
        "}"
      ],
      "metadata": {
        "id": "D7eBU8BAewPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "from torchcrf import CRF\n",
        "\n",
        "\n",
        "class DeepPunctuation(nn.Module):\n",
        "    def __init__(self, pretrained_model, freeze_bert=False, lstm_dim=-1):\n",
        "        super(DeepPunctuation, self).__init__()\n",
        "        self.output_dim = len(punctuation_dict)\n",
        "        self.bert_layer = MODELS[pretrained_model][0].from_pretrained(pretrained_model)\n",
        "        # Freeze bert layers\n",
        "        if freeze_bert:\n",
        "            for p in self.bert_layer.parameters():\n",
        "                p.requires_grad = False\n",
        "        bert_dim = MODELS[pretrained_model][2]\n",
        "        if lstm_dim == -1:\n",
        "            hidden_size = bert_dim\n",
        "        else:\n",
        "            hidden_size = lstm_dim\n",
        "        self.lstm = nn.LSTM(input_size=bert_dim, hidden_size=hidden_size, num_layers=1, bidirectional=True)\n",
        "        self.linear = nn.Linear(in_features=hidden_size*2, out_features=len(punctuation_dict))\n",
        "\n",
        "    def forward(self, x, attn_masks):\n",
        "        if len(x.shape) == 1:\n",
        "            x = x.view(1, x.shape[0])  # add dummy batch for single sample\n",
        "        # (B, N, E) -> (B, N, E)\n",
        "        x = self.bert_layer(x, attention_mask=attn_masks)[0]\n",
        "        # (B, N, E) -> (N, B, E)\n",
        "        x = torch.transpose(x, 0, 1)\n",
        "        x, (_, _) = self.lstm(x)\n",
        "        # (N, B, E) -> (B, N, E)\n",
        "        x = torch.transpose(x, 0, 1)\n",
        "        x = self.linear(x)\n",
        "        return x\n",
        "\n",
        "class DeepPunctuationCRF(nn.Module):\n",
        "    def __init__(self, pretrained_model, freeze_bert=False, lstm_dim=-1):\n",
        "        super(DeepPunctuationCRF, self).__init__()\n",
        "        self.bert_lstm = DeepPunctuation(pretrained_model, freeze_bert, lstm_dim)\n",
        "        self.crf = CRF(len(punctuation_dict), batch_first=True)\n",
        "\n",
        "    def log_likelihood(self, x, attn_masks, y):\n",
        "        x = self.bert_lstm(x, attn_masks)\n",
        "        attn_masks = attn_masks.byte()\n",
        "        return -self.crf(x, y, mask=attn_masks, reduction='token_mean')\n",
        "\n",
        "    def forward(self, x, attn_masks, y):\n",
        "        if len(x.shape) == 1:\n",
        "            x = x.view(1, x.shape[0])  # add dummy batch for single sample\n",
        "        x = self.bert_lstm(x, attn_masks)\n",
        "        attn_masks = attn_masks.byte()\n",
        "        dec_out = self.crf.decode(x, mask=attn_masks)\n",
        "        y_pred = torch.zeros(y.shape).long().to(y.device)\n",
        "        for i in range(len(dec_out)):\n",
        "            y_pred[i, :len(dec_out[i])] = torch.tensor(dec_out[i]).to(y.device)\n",
        "        return y_pred"
      ],
      "metadata": {
        "id": "IkesWmuXe8Qw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "id": "6nYB7hwSedeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import torch\n",
        "\n",
        "import argparse\n",
        "\n",
        "lstm_dim=-1\n",
        "use_crf=False\n",
        "language='bn'\n",
        "in_file='data/test_bn.txt'\n",
        "weight_path='/content/punctuation-restoration/xlm-roberta-large-bn.pt'\n",
        "sequence_length=256\n",
        "out_file='data/test_en_out.txt'\n",
        "pretrained_model='xlm-roberta-large'\n",
        "\n",
        "# tokenizer\n",
        "tokenizer = MODELS[pretrained_model][1].from_pretrained(pretrained_model)\n",
        "token_style = MODELS[pretrained_model][3]\n",
        "\n",
        "# logs\n",
        "model_save_path = weight_path\n",
        "\n",
        "# Model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "if use_crf:\n",
        "    deep_punctuation = DeepPunctuationCRF(pretrained_model, freeze_bert=False, lstm_dim=lstm_dim)\n",
        "else:\n",
        "    deep_punctuation = DeepPunctuation(pretrained_model, freeze_bert=False, lstm_dim=lstm_dim)\n",
        "deep_punctuation.to(device)\n",
        "\n",
        "deep_punctuation.load_state_dict(torch.load(model_save_path))\n",
        "deep_punctuation.eval()"
      ],
      "metadata": {
        "id": "P0JU8A4zaNGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(text):\n",
        "\n",
        "    words_original_case = text.split()\n",
        "    words = text.split()\n",
        "\n",
        "    word_pos = 0\n",
        "    sequence_len = sequence_length\n",
        "    result = \"\"\n",
        "    decode_idx = 0\n",
        "    punctuation_map = {0: '', 1: ',', 2: '।', 3: '?'}\n",
        "    if language != 'en':\n",
        "        punctuation_map[2] = '।'\n",
        "\n",
        "    while word_pos < len(words):\n",
        "        x = [TOKEN_IDX[token_style]['START_SEQ']]\n",
        "        y_mask = [0]\n",
        "\n",
        "        while len(x) < sequence_len and word_pos < len(words):\n",
        "            tokens = tokenizer.tokenize(words[word_pos])\n",
        "            if len(tokens) + len(x) >= sequence_len:\n",
        "                break\n",
        "            else:\n",
        "                for i in range(len(tokens) - 1):\n",
        "                    x.append(tokenizer.convert_tokens_to_ids(tokens[i]))\n",
        "                    y_mask.append(0)\n",
        "                x.append(tokenizer.convert_tokens_to_ids(tokens[-1]))\n",
        "                y_mask.append(1)\n",
        "                word_pos += 1\n",
        "        x.append(TOKEN_IDX[token_style]['END_SEQ'])\n",
        "        y_mask.append(0)\n",
        "        if len(x) < sequence_len:\n",
        "            x = x + [TOKEN_IDX[token_style]['PAD'] for _ in range(sequence_len - len(x))]\n",
        "            y_mask = y_mask + [0 for _ in range(sequence_len - len(y_mask))]\n",
        "        attn_mask = [1 if token != TOKEN_IDX[token_style]['PAD'] else 0 for token in x]\n",
        "\n",
        "        x = torch.tensor(x).reshape(1,-1)\n",
        "        y_mask = torch.tensor(y_mask)\n",
        "        attn_mask = torch.tensor(attn_mask).reshape(1,-1)\n",
        "        x, attn_mask, y_mask = x.to(device), attn_mask.to(device), y_mask.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if use_crf:\n",
        "                y = torch.zeros(x.shape[0])\n",
        "                y_predict = deep_punctuation(x, attn_mask, y)\n",
        "                y_predict = y_predict.view(-1)\n",
        "            else:\n",
        "                y_predict = deep_punctuation(x, attn_mask)\n",
        "                y_predict = y_predict.view(-1, y_predict.shape[2])\n",
        "                y_predict = torch.argmax(y_predict, dim=1).view(-1)\n",
        "        for i in range(y_mask.shape[0]):\n",
        "            if y_mask[i] == 1:\n",
        "                result += words_original_case[decode_idx] + punctuation_map[y_predict[i].item()] + ' '\n",
        "                decode_idx += 1\n",
        "    return result"
      ],
      "metadata": {
        "id": "Bln1xkvFvy_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!!pip install pandarallel"
      ],
      "metadata": {
        "id": "Cr6eP45VKG7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from IPython import display as ipd\n",
        "\n",
        "from pandarallel import pandarallel\n",
        "pandarallel.initialize(progress_bar=True,nb_workers=8)\n",
        "tqdm.pandas()\n",
        "\n"
      ],
      "metadata": {
        "id": "YEvd2IUajneJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ysDJQtq6vgCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/punctuation-restoration/2.csv')"
      ],
      "metadata": {
        "id": "fFEUmLYsies8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def space_remover(x):\n",
        "    return \" \".join(x.split())"
      ],
      "metadata": {
        "id": "QfYMGuT3vgrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df =df.drop('sentence',axis=1)"
      ],
      "metadata": {
        "id": "ZGRbFCuBzG0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.spelling = df.spelling.apply(space_remover)"
      ],
      "metadata": {
        "id": "XqtTXF5jipwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "BQRMKzEoH475"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.rename(columns={'spelling':'predicted'})"
      ],
      "metadata": {
        "id": "fa3nvhBJKooJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['predicted']=df['predicted'].str.replace('।','')"
      ],
      "metadata": {
        "id": "zz96ceQfJt-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "_rTtrqMgLL8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['punctuation'] = df['predicted'].apply(lambda x : inference(x))"
      ],
      "metadata": {
        "id": "2hco4ZWG0q1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = df.drop('predicted',axis=1)"
      ],
      "metadata": {
        "id": "hndnW591X5qN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.to_csv('final_with_punctuation_needs_cleaning.csv')"
      ],
      "metadata": {
        "id": "OvB54z2AX1jY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['punctuation'][7612]"
      ],
      "metadata": {
        "id": "nRYa8iQxmgzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[df.punctuation.str.find('?') != -1]"
      ],
      "metadata": {
        "id": "Sj4zuMQqH8-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['punctuation_truncated'] =df.punctuation.apply(lambda x : x.strip())\n",
        "df['punctuation_truncated'] =df.punctuation.apply(space_remover)"
      ],
      "metadata": {
        "id": "5RFYL3_nQLh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['punctuation_truncated'] = df['punctuation_truncated'].str.replace('।', '')\n",
        "df['punctuation_truncated'] = df['punctuation_truncated'].str.replace('.', '')"
      ],
      "metadata": {
        "id": "k6-gO9hLn-7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(df)):\n",
        "    if df['punctuation_truncated'][i][-1] == ',':\n",
        "      df['punctuation_truncated'][i] = df['punctuation_truncated'][i][:-1]"
      ],
      "metadata": {
        "id": "DpUdtf7osN4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(df)):\n",
        "    if df['punctuation_truncated'][i][-1] not in [',','?']:\n",
        "        df['punctuation_truncated'][i] = df['punctuation_truncated'][i] + '।'"
      ],
      "metadata": {
        "id": "4T063weToc8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=df.drop(['predicted','punctuation'],axis=1)"
      ],
      "metadata": {
        "id": "udiCxkvPQt2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=df.rename(columns={\"punctuation_truncated\":\"sentence\"})"
      ],
      "metadata": {
        "id": "_kirk9quRUsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(df)):\n",
        "    if df['sentence'][i][-1] in [',','?']:\n",
        "      print(df['sentence'][i])"
      ],
      "metadata": {
        "id": "w7I56AQipL0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"final.csv\",index=False)"
      ],
      "metadata": {
        "id": "ooeDxiObPg9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y1QvftAvryPj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}